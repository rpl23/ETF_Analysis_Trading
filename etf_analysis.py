# -*- coding: utf-8 -*-
"""ETF ANALYSIS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O8hqVu7Y09BnTokoY7eKlD9u1gb4yRiM

##IMPORT ETF DATA

Analyzing SPY, QQQ, IWM, TLT, GLD, XLF, XLK, XLE
"""

import yfinance as yf
import pandas as pd
import numpy as np
from datetime import datetime

# Define your ETF list
etf_list = ['SPY', 'QQQ', 'IWM', 'TLT', 'GLD', 'XLF', 'XLK', 'XLE']

def fetch_etf_data(etf_list, start_date, end_date):
    # Create an empty DataFrame to store all ETF data
    combined_df = pd.DataFrame()

    for etf in etf_list:
        try:
            # Download data for each ETF separately
            df = yf.download(etf, start=start_date, end=end_date)

            # Keep only the Close prices and volume
            df = df[['Close', 'Volume']]

            # Rename columns with simpler names
            df.columns = [f'{etf}_Close', f'{etf}_Volume']

            # If combined_df is empty, assign first ETF data
            if combined_df.empty:
                combined_df = df
            else:
                # Join with existing data
                combined_df = combined_df.join(df)

            print(f"Successfully downloaded data for {etf}")

        except Exception as e:
            print(f"Error fetching data for {etf}: {e}")

    return combined_df

# Fetch the data again
start_date = "2018-01-01"
end_date = datetime.now().strftime("%Y-%m-%d")
data = fetch_etf_data(etf_list, start_date, end_date)

def create_features(df, window_short=5, window_long=20):
    """
    Create features for all ETFs
    """
    features = pd.DataFrame(index=df.index)

    # Process each ETF
    for etf in etf_list:
        # Get the closing price and volume columns
        price_col = f'{etf}_Close'
        vol_col = f'{etf}_Volume'

        # Returns
        features[f'{etf}_returns'] = df[price_col].pct_change()

        # Moving averages
        features[f'{etf}_MA_short'] = df[price_col].rolling(window=window_short).mean()
        features[f'{etf}_MA_long'] = df[price_col].rolling(window=window_long).mean()

        # Volatility
        features[f'{etf}_volatility'] = features[f'{etf}_returns'].rolling(window=window_short).std()

        # Volume features
        features[f'{etf}_volume_MA'] = df[vol_col].rolling(window=window_short).mean()
        features[f'{etf}_volume_ratio'] = df[vol_col] / features[f'{etf}_volume_MA']

    # Cross-ETF features
    # Relative strength vs SPY
    spy_returns = features['SPY_returns']
    for etf in [e for e in etf_list if e != 'SPY']:
        features[f'{etf}_rel_strength'] = features[f'{etf}_returns'] - spy_returns

    # Drop NaN values
    features = features.dropna()

    return features

# Create features
features = create_features(data)

# Check the results
print("\nFeatures shape:", features.shape)
print("\nFirst few feature columns:", list(features.columns)[:5])

"""##DATA PREP FOR TRAINING

Creates target variables (5-day future returns for each ETF)
Splits the data into training and testing sets while preserving time order
Scales the features
"""

def prepare_training_data(features, target_days=5):
    """
    Prepare features and target variables for model training
    """
    # Create target variables (future returns)
    targets = pd.DataFrame(index=features.index)

    for etf in etf_list:
        # Calculate future returns for each ETF
        price_col = f'{etf}_returns'
        future_returns = features[price_col].shift(-target_days)  # Future 5-day returns
        targets[f'{etf}_target'] = future_returns

    # Remove the last target_days rows since we won't have targets for them
    features = features[:-target_days]
    targets = targets[:-target_days]

    # Remove any remaining NaN values
    valid_indices = ~targets.isnull().any(axis=1)
    features = features[valid_indices]
    targets = targets[valid_indices]

    return features, targets

# Prepare the data
X, y = prepare_training_data(features)

# Split into training and testing sets
from sklearn.model_selection import train_test_split

# Use the last 20% of data for testing (to maintain temporal order)
train_size = int(len(X) * 0.8)
X_train = X[:train_size]
X_test = X[train_size:]
y_train = y[:train_size]
y_test = y[train_size:]

# Scale the features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Training set shape:", X_train_scaled.shape)
print("Testing set shape:", X_test_scaled.shape)

"""##RANDOM FOREST

Train a Random Forest model to predict returns for all ETFs
Evaluate the model's performance on both training and test sets
Show the most important features
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Modify the Random Forest parameters
model = RandomForestRegressor(
    n_estimators=100,
    max_depth=6,  # Reduced from 10
    min_samples_leaf=10,  # Added parameter
    min_samples_split=10,  # Increased from 5
    random_state=42
)

# Train the model
model.fit(X_train_scaled, y_train)

# Make predictions
y_pred_train = model.predict(X_train_scaled)
y_pred_test = model.predict(X_test_scaled)

# Evaluate model performance
def evaluate_predictions(y_true, y_pred, set_name="Test"):
    results = {}
    for i, etf in enumerate(etf_list):
        mse = mean_squared_error(y_true[f'{etf}_target'], y_pred[:, i])
        r2 = r2_score(y_true[f'{etf}_target'], y_pred[:, i])
        results[etf] = {'MSE': mse, 'R2': r2}
        print(f"{set_name} metrics for {etf}:")
        print(f"MSE: {mse:.6f}")
        print(f"R2: {r2:.4f}")
        print("-" * 30)
    return results

# Evaluate the model
print("Training Set Results:")
train_results = evaluate_predictions(y_train, y_pred_train, "Training")
print("\nTest Set Results:")
test_results = evaluate_predictions(y_test, y_pred_test, "Test")

# Get feature importance
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
})
feature_importance = feature_importance.sort_values('importance', ascending=False)

# Keep only the most important features
top_features = feature_importance.head(20)['feature'].tolist()
X_train_selected = X_train_scaled[:, [list(X.columns).index(col) for col in top_features]]
X_test_selected = X_test_scaled[:, [list(X.columns).index(col) for col in top_features]]

# Print top 10 most important features
print("\nTop 10 Most Important Features:")
print(feature_importance.head(10))

"""##TIME SERIES CROSS-VALIDATION"""

from sklearn.model_selection import TimeSeriesSplit

def time_series_cv(X, y, model, n_splits=5):
    tscv = TimeSeriesSplit(n_splits=n_splits)
    cv_scores = []

    for train_idx, val_idx in tscv.split(X):
        # Split data
        X_tr, X_val = X[train_idx], X[val_idx]
        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]

        # Train model
        model.fit(X_tr, y_tr)

        # Evaluate
        val_r2 = r2_score(y_val, model.predict(X_val))
        cv_scores.append(val_r2)

    return cv_scores

# Create a simpler model
simple_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=4,
    min_samples_leaf=20,
    min_samples_split=20,
    random_state=42
)

# Run time-series cross-validation
cv_scores = time_series_cv(X_train_scaled, y_train, simple_model)
print("Cross-validation R² scores:", cv_scores)
print("Mean CV R²:", np.mean(cv_scores))

"""##BINARY CLASSIFICATION (UP/DOWN PRICE MOVEMENT)
20 day horizon
"""

def create_regime_features(df, window=20):
    features = pd.DataFrame(index=df.index)

    # Market regime indicators
    spy_price = df['SPY_Close']
    spy_vol = df['SPY_Volume']

    # Trend features
    features['market_trend'] = (spy_price > spy_price.rolling(window=window).mean()).astype(int)

    # Volatility regime
    spy_returns = spy_price.pct_change()
    vol = spy_returns.rolling(window=window).std()
    features['high_vol_regime'] = (vol > vol.rolling(window=window).mean()).astype(int)

    # Volume regime
    features['high_volume_regime'] = (spy_vol > spy_vol.rolling(window=window).mean()).astype(int)

    # Sector rotation features
    for etf in ['XLF', 'XLK', 'XLE']:
        etf_returns = df[f'{etf}_Close'].pct_change()
        features[f'{etf}_relative_strength'] = (etf_returns.rolling(window=window).mean()
                                              - spy_returns.rolling(window=window).mean())

    return features.dropna()

# Convert to classification problem
def prepare_binary_targets(features, target_days=20):
    targets = pd.DataFrame(index=features.index)

    for etf in etf_list:
        price_col = f'{etf}_Close'
        future_returns = (data[price_col].shift(-target_days) > data[price_col]).astype(int)
        targets[f'{etf}_target'] = future_returns

    return targets.dropna()

# Create new features and targets
regime_features = create_regime_features(data)
binary_targets = prepare_binary_targets(data)

# Combine features
X = pd.concat([features, regime_features], axis=1).dropna()
y = binary_targets.loc[X.index]

# Align data
common_idx = X.index.intersection(y.index)
X = X.loc[common_idx]
y = y.loc[common_idx]

# Train a classifier
from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(
    n_estimators=100,
    max_depth=5,
    min_samples_leaf=50,
    random_state=42
)

# Time series cross validation
def evaluate_binary_predictions(y_true, y_pred, etf):
    from sklearn.metrics import accuracy_score, precision_score, recall_score
    return {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred)
    }

print("Training binary classifier...")
# Use last 20% for testing
split_idx = int(len(X) * 0.8)
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train and evaluate
clf.fit(X_train_scaled, y_train)

# Evaluate each ETF
for etf in etf_list:
    y_pred = clf.predict(X_test_scaled)
    metrics = evaluate_binary_predictions(y_test[f'{etf}_target'], y_pred[:, list(y_test.columns).index(f'{etf}_target')], etf)
    print(f"\nMetrics for {etf}:")
    for metric, value in metrics.items():
        print(f"{metric}: {value:.4f}")

"""High Precision, Low Recall Pattern (most ETFs):


SPY, QQQ, IWM, GLD, XLF, XLK show very high precision (>85%) but low recall (~10-23%)
This means when the model predicts an upward movement, it's usually right, but it misses many actual upward movements
The model is being very conservative in its predictions


Special Cases:


TLT (Bonds): 0.0 precision/recall suggests the model isn't making any positive predictions
XLE (Energy): Perfect recall (1.0) but lower precision (0.48) means it's predicting up movement too frequently
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import accuracy_score, precision_score, recall_score

# Dictionary to store models for each ETF
etf_models = {}

def predict_with_threshold(model, X, threshold=0.4):
    probas = model.predict_proba(X)
    return (probas[:, 1] > threshold).astype(int)

# Train separate model for each ETF
for etf in etf_list:
    print(f"\nTraining model for {etf}...")

    # Get target for this ETF
    y_train_etf = y_train[f'{etf}_target']
    y_test_etf = y_test[f'{etf}_target']

    # Create base classifier first to get feature importance
    base_clf = RandomForestClassifier(
        n_estimators=100,
        max_depth=5,
        min_samples_leaf=30,
        class_weight='balanced',
        random_state=42
    )

    # Fit base classifier for feature importance
    base_clf.fit(X_train_scaled, y_train_etf)

    # Create and fit calibrated classifier
    calibrated_clf = CalibratedClassifierCV(
        RandomForestClassifier(
            n_estimators=100,
            max_depth=5,
            min_samples_leaf=30,
            class_weight='balanced',
            random_state=42
        ),
        cv=5
    )
    calibrated_clf.fit(X_train_scaled, y_train_etf)

    # Store calibrated model
    etf_models[etf] = calibrated_clf

    # Make predictions using lower threshold
    y_pred = predict_with_threshold(calibrated_clf, X_test_scaled, threshold=0.3)

    # Calculate metrics
    accuracy = accuracy_score(y_test_etf, y_pred)
    precision = precision_score(y_test_etf, y_pred)
    recall = recall_score(y_test_etf, y_pred)

    print(f"Metrics for {etf}:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")

    # Feature importance from base classifier
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': base_clf.feature_importances_
    })
    print(f"\nTop 5 Important Features for {etf}:")
    print(feature_importance.sort_values('importance', ascending=False).head(5))

# analyze how often different ETFs agree on direction
test_predictions = pd.DataFrame(index=X_test.index)
for etf in etf_list:
    test_predictions[etf] = predict_with_threshold(etf_models[etf], X_test_scaled, threshold=0.3)

# Calculate agreement matrix
agreement_matrix = test_predictions.corr()
print("\nPrediction Agreement Matrix:")
print(agreement_matrix)

"""Performance Patterns:


Best performing: SPY (76.1%), QQQ (73%), XLF (72.4%)
Moderate performing: GLD (69.6%), XLK (68.7%), IWM (59.7%)
Challenging: TLT (51.8%), XLE (48.2%)


Important Feature Patterns:


Moving Averages dominate the top features for all ETFs
Cross-ETF indicators are important (e.g., XLE_MA_long is important for SPY)

Specific patterns:

GLD is most influenced by its own MAs
XLE is heavily influenced by broad market indicators (SPY, QQQ)
TLT (bonds) is influenced by GLD and XLE, suggesting sensitivity to inflation/risk indicators

##ADAPTIVE MODEL:
Creates new adaptive features based on market regimes
Combines them with our original features
Uses enhanced RandomForest parameters
Provides comprehensive metrics for each ETF
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score

# Create adaptive features
def create_adaptive_features(df, window_short=5, window_long=20):
    features = pd.DataFrame(index=df.index)

    # Market regime features
    for etf in etf_list:
        price = df[f'{etf}_Close']

        # Price momentum
        returns = price.pct_change()
        features[f'{etf}_trend_strength'] = (
            returns.rolling(window=window_long).mean() /
            returns.rolling(window=window_long).std()
        )

        # Distance from moving averages
        ma_short = price.rolling(window=window_short).mean()
        ma_long = price.rolling(window=window_long).mean()
        features[f'{etf}_ma_distance'] = (ma_short - ma_long) / ma_long

        # Volatility regime
        features[f'{etf}_vol_regime'] = returns.rolling(window=window_long).std()

    # Cross-asset relationships
    for etf in [e for e in etf_list if e != 'SPY']:
        features[f'{etf}_spy_ratio'] = (
            df[f'{etf}_Close'] / df['SPY_Close']
        ).pct_change(window_long)

    return features.dropna()

# Create new features
adaptive_features = create_adaptive_features(data)

# Combine with existing features
X_combined = pd.concat([X, adaptive_features], axis=1)

# Scale the features
scaler = StandardScaler()
X_train_combined = scaler.fit_transform(X_combined[:split_idx])
X_test_combined = scaler.transform(X_combined[split_idx:])

# Train models with new features
for etf in etf_list:
    print(f"\nTraining enhanced model for {etf}...")

    # Get target for this ETF
    y_train_etf = y_train[f'{etf}_target']
    y_test_etf = y_test[f'{etf}_target']

    # Create classifier with adjusted parameters
    clf = RandomForestClassifier(
        n_estimators=200,
        max_depth=6,
        min_samples_leaf=20,
        class_weight='balanced_subsample',
        random_state=42
    )

    # Train model
    clf.fit(X_train_combined, y_train_etf)

    # Make predictions
    y_pred = clf.predict(X_test_combined)

    # Calculate metrics
    accuracy = accuracy_score(y_test_etf, y_pred)
    precision = precision_score(y_test_etf, y_pred)
    recall = recall_score(y_test_etf, y_pred)

    print(f"Enhanced Metrics for {etf}:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")

    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': X_combined.columns,
        'importance': clf.feature_importances_
    })
    print(f"\nTop 5 Important Features for {etf}:")
    print(feature_importance.sort_values('importance', ascending=False).head(5))

"""Performance Characteristics:


Most ETFs show very high precision (85-100%) but low recall (4-13%)
XLE is the exception with more balanced metrics (48.5% precision, 88.3% recall)
TLT (bonds) shows particularly poor performance with 0% precision/recall


Feature Importance Patterns:


Moving averages remain the most important predictors
Long-term MAs (MA_long) appear more frequently than short-term
Cross-ETF relationships are significant (e.g., XLE_MA_long affects SPY)
New volatility regime features show up for some ETFs (e.g., XLE_vol_regime)

##FINAL METHOD
Uses two models with different biases
Combines their predictions with weights
Adjusts class weights to balance precision/recall
Uses a lower threshold for final predictions
"""

def train_balanced_model(X_train, y_train, X_test, y_test, etf):
    # Create two models with different objectives
    conservative_clf = RandomForestClassifier(
        n_estimators=200,
        max_depth=4,
        min_samples_leaf=30,
        class_weight={0:1, 1:2},  # Slight bias towards positive class
        random_state=42
    )

    aggressive_clf = RandomForestClassifier(
        n_estimators=200,
        max_depth=6,
        min_samples_leaf=20,
        class_weight={0:1, 1:4},  # Stronger bias towards positive class
        random_state=42
    )

    # Train both models
    y_train_etf = y_train[f'{etf}_target']
    y_test_etf = y_test[f'{etf}_target']

    conservative_clf.fit(X_train, y_train_etf)
    aggressive_clf.fit(X_train, y_train_etf)

    # Combine predictions
    conservative_prob = conservative_clf.predict_proba(X_test)[:, 1]
    aggressive_prob = aggressive_clf.predict_proba(X_test)[:, 1]

    # Average probabilities with weights
    final_prob = (0.7 * conservative_prob + 0.3 * aggressive_prob)
    y_pred = (final_prob > 0.4).astype(int)

    return y_pred, conservative_clf

# Train new models
for etf in etf_list:
    print(f"\nTraining balanced model for {etf}...")
    y_pred, model = train_balanced_model(X_train_combined, y_train,
                                       X_test_combined, y_test, etf)

    # Calculate metrics
    y_test_etf = y_test[f'{etf}_target']
    accuracy = accuracy_score(y_test_etf, y_pred)
    precision = precision_score(y_test_etf, y_pred)
    recall = recall_score(y_test_etf, y_pred)

    print(f"Balanced Metrics for {etf}:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")

"""Best Performers:


GLD (Gold): 72.1% accuracy, excellent recall (93.9%)
SPY (S&P 500): 68.2% accuracy, well-balanced metrics (81.8% precision, 74.8% recall)
XLF (Financials): 69.3% accuracy, strong precision (89.4%)
XLK (Technology): 65.1% accuracy, high recall (91.8%)


Moderate Performers:


QQQ (Nasdaq): 60.9% accuracy, decent balance (78.9% precision, 63.3% recall)


Challenging Performers:


IWM (Small Caps): Lower accuracy (43.4%), struggles with recall
TLT (Bonds): Poor recall (0.5%), suggesting difficulty predicting upward movements
XLE (Energy): High recall but lower precision, suggesting over-prediction of upward movements

Key Insights:

Asset Class Patterns:

Traditional safe-haven assets (GLD) show strong predictability
Large-cap indices (SPY, QQQ) are more predictable than small-caps (IWM)
Bonds (TLT) remain challenging to predict


Sector Patterns:

Financial sector (XLF) shows strong predictability
Tech sector (XLK) predictions favor recall over precision
Energy sector (XLE) shows high sensitivity but lower accuracy

##TRADING STRATEGY
Uses the top performing ETFs (GLD, SPY, XLF, XLK)
Implements position sizing based on prediction confidence
Includes risk management rules
"""

def create_trading_strategy(data, models, X_test_combined, confidence_threshold=0.6):
    """
    Create trading signals based on model predictions
    """
    # Create DataFrame with proper index
    predictions = pd.DataFrame(index=data.index[-len(X_test_combined):])

    for etf in ['SPY', 'GLD', 'XLF', 'XLK']:
        model = models[etf]
        # Get prediction probabilities
        probas = model.predict_proba(X_test_combined)
        predictions[f'{etf}_prob'] = probas[:, 1]
        predictions[f'{etf}_signal'] = (probas[:, 1] > confidence_threshold).astype(int)

    # Calculate position sizes based on confidence
    portfolio_size = 100000  # Example $100k portfolio
    max_position_size = 0.25  # Maximum 25% in any single position

    for etf in ['SPY', 'GLD', 'XLF', 'XLK']:
        confidence = predictions[f'{etf}_prob']
        # Scale position size by confidence above threshold
        predictions[f'{etf}_position'] = np.where(
            confidence > confidence_threshold,
            portfolio_size * max_position_size * (confidence - confidence_threshold) / (1 - confidence_threshold),
            0
        )

    # Add risk management rules
    predictions['total_exposure'] = predictions[[f'{etf}_position' for etf in ['SPY', 'GLD', 'XLF', 'XLK']]].sum(axis=1)
    predictions['cash'] = portfolio_size - predictions['total_exposure']

    return predictions

def calculate_strategy_returns(predictions, data, lookforward_period=5):
    """
    Calculate strategy returns based on positions and actual price changes
    """
    strategy_returns = pd.DataFrame(index=predictions.index)

    for etf in ['SPY', 'GLD', 'XLF', 'XLK']:
        # Calculate forward returns
        price_col = f'{etf}_Close'
        future_prices = data.loc[predictions.index, price_col].shift(-lookforward_period)
        current_prices = data.loc[predictions.index, price_col]
        returns = (future_prices - current_prices) / current_prices

        # Calculate position returns
        strategy_returns[f'{etf}_return'] = returns * predictions[f'{etf}_position'] / 100000

    strategy_returns['total_return'] = strategy_returns.sum(axis=1)
    strategy_returns['cumulative_return'] = (1 + strategy_returns['total_return']).cumprod()

    return strategy_returns

def calculate_risk_metrics(returns):
    """
    Calculate key risk metrics for the strategy
    """
    metrics = {
        'Annualized Return': returns['total_return'].mean() * 252,
        'Annualized Volatility': returns['total_return'].std() * np.sqrt(252),
        'Sharpe Ratio': returns['total_return'].mean() / returns['total_return'].std() * np.sqrt(252) if returns['total_return'].std() != 0 else 0,
        'Max Drawdown': (1 - returns['cumulative_return'] / returns['cumulative_return'].cummax()).max(),
        'Win Rate': (returns['total_return'] > 0).mean()
    }
    return metrics

# Store models from previous training
etf_models = {}
for etf in ['SPY', 'GLD', 'XLF', 'XLK']:
    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=4,
        min_samples_leaf=30,
        class_weight={0:1, 1:2},
        random_state=42
    )
    y_train_etf = y_train[f'{etf}_target']
    model.fit(X_train_combined, y_train_etf)
    etf_models[etf] = model

# Implement strategy
predictions = create_trading_strategy(data, etf_models, X_test_combined)
strategy_returns = calculate_strategy_returns(predictions, data)
risk_metrics = calculate_risk_metrics(strategy_returns)

# Print results
print("\nStrategy Risk Metrics:")
for metric, value in risk_metrics.items():
    print(f"{metric}: {value:.4f}")

# Print current positions
latest_date = predictions.index[-1]
print("\nCurrent Positions:")
for etf in ['SPY', 'GLD', 'XLF', 'XLK']:
    position = predictions.loc[latest_date, f'{etf}_position']
    confidence = predictions.loc[latest_date, f'{etf}_prob']
    if position > 0:
        print(f"{etf}: ${position:.2f} (Confidence: {confidence:.2%})")

print(f"Cash: ${predictions.loc[latest_date, 'cash']:.2f}")

# Plot cumulative returns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(strategy_returns.index, strategy_returns['cumulative_return'])
plt.title('Strategy Cumulative Returns')
plt.xlabel('Date')
plt.ylabel('Cumulative Return')
plt.grid(True)
plt.show()
